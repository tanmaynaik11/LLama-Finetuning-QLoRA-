# ğŸ§  Fine-Tuning & Evaluation of LLaMA 3.2 1B with QLoRA, LangFlow, and DeepEval

This project fine-tunes Metaâ€™s LLaMA 3.2 1B using **QLoRA** and evaluates instruction dataset quality using **LLM-as-a-judge**, integrated with **LangFlow**, **Docling**, and **DeepEval**. It streamlines dataset scoring, model tuning, and response evaluation in a lightweight and efficient workflow suitable for consumer GPUs.

---

## ğŸš€ Project Highlights

- âœ… Fine-tuned **LLaMA 3.2 1B** using **4-bit QLoRA + LoRA adapters** on a single 24 GB GPU
- âœ… Built an LLM-powered auto-evaluator pipeline using **LiteLLM + Pydantic schemas**
- âœ… Filtered **5,000+ instruction-response pairs**, retaining **~91% high-quality data**
- âœ… Integrated **LangFlow** and **Docling** for visual scoring, auditing, and real-time analysis
- âœ… Evaluated fine-tuned model using **DeepEval** across fluency, coherence, and factuality

---

## ğŸ› ï¸ Tech Stack

| Tool        | Purpose                          |
|-------------|----------------------------------|
| ğŸ¤– LLaMA 3.2 1B | Base model for fine-tuning     |
| ğŸ§  QLoRA + LoRA | Efficient adapter fine-tuning  |
| âš™ï¸ Transformers | Tokenizer & model management    |
| ğŸ”— LiteLLM     | LLM-as-a-judge evaluation API  |
| ğŸŒ LangFlow    | Flow-based interface for prompt modeling & scoring |
| ğŸ“Š Docling     | Visual analysis of record quality |
| ğŸ“ DeepEval    | Structured LLM evaluation metrics |
| ğŸ Python      | Core scripting and dataset curation |
| ğŸ’¾ Datasets    | Custom instruction dataset      |

---

## ğŸ“ˆ Evaluation Metrics

| Metric               | Value                    |
|----------------------|--------------------------|
| ğŸ¯ Match Accuracy     | 92.4% (DeepEval)         |
| ğŸ’¬ Coherence Score    | 3.9 / 5 (DeepEval)       |
| ğŸ§ª Instruction Retention | ~3,800 of 5,000+ pairs |
| âš¡ Inference Speed    | 2.3Ã— faster vs. FP16     |
| ğŸ“š Fluency Rating     | 4.7 / 5 peer-reviewed    |


